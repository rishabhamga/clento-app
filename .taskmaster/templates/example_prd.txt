
The plan will also cover:

* Dynamic website analysis using AI for onboarding
* Campaign flows with conditional workflows (e.g., connect on LinkedIn, follow-up by email)
* Enrichment and personalization via LLMs
* Technical architecture with fast dev tools (Supabase, Clerk, SaaS-UI.dev, etc.)
* A clickable architecture diagram

I’ll get started and update you when everything is ready.


# AI SDR Platform Development Plan

## Overview and Vision

We plan to build an **AI-powered Sales Development Representative (SDR) platform** that automates lead generation and multichannel outreach with personalized messaging – essentially a “virtual SDR” similar to Artisan AI, Seamless.ai, and AiSDR. The platform will **find ideal leads, enrich their data, and engage them via LinkedIn and email with AI-crafted messages**, while tracking responses. This **“AI SDR” agent** will handle top-of-funnel tasks (prospecting, outreach, follow-ups) so the user can focus on closing deals. The project must be completed by a single developer in \~30 days, so we will leverage rapid-development tools (Supabase, Clerk, SaaS UI, etc.) and pre-built APIs to accelerate progress. The end product should deliver the same core functionality as Artisan’s AI SDR but with an even better UI/UX and an “AI-first” design in every feature.

**Key Features to Implement:**

* **Lead Sourcing:** Search a massive B2B database for contacts/companies matching user-defined filters (e.g. industry, title, location). Use ZoomInfo as primary data source (300M+ contacts with detailed firmographics), with fallbacks to Apollo.io or Clearbit for additional coverage. Also identify website visitors (using ZoomInfo WebSights or Clearbit Reveal) to turn anonymous traffic into leads. Optionally, integrate Google Maps API to find local businesses by category for hyper-local prospecting.
* **Lead Enrichment:** Automatically enrich each lead with **deep data** – e.g. company size, industry, technographics, socials, recent news, etc. – via ZoomInfo’s Enrich API and supplementary sources. An AI “research assistant” will analyze each prospect’s digital footprint (LinkedIn, news, social media) to extract **relevant talking points** for personalization.
* **Website Analysis for Onboarding:** Upon signup, the user enters their company website. An **AI model will analyze the website** to understand the product, value proposition, and key selling points. This helps tailor outreach: the AI SDR will incorporate the company’s unique value in messages. (This uses an LLM to summarize product benefits from the site content.)
* **Personalized AI Outreach:** The platform generates highly personalized outreach messages (emails, LinkedIn connect notes, etc.) for each lead using an LLM (e.g. GPT-4). Messages will reference the lead’s context and the user’s product value prop. We’ll allow the user to input a custom “hook” or angle to guide the AI. The AI will write initial cold emails, LinkedIn invites, and follow-ups, mimicking the user’s tone/brand voice as much as possible.
* **Multichannel Sequence Automation:** Users can create outreach *campaigns* with multi-step sequences. For example: Day 1 send a LinkedIn connection request; if accepted, immediately send a personalized LinkedIn message; if no LinkedIn accept in X days, send an email; if no response, send a follow-up email, etc. The workflow engine will support conditional steps and delays (similar to a cadence/playbook). This ensures leads are contacted on multiple channels intelligently – a key differentiator of AI SDRs.
* **Email and Domain Warm-up:** All email outreach will use a dedicated sending domain (or subdomain) to protect the user’s primary domain reputation. We will integrate with the existing email infrastructure the user has set up (e.g. Google Workspace accounts on warmed domains). The system will monitor deliverability – e.g. automatic domain warm-up, inbox health checks, and bounce tracking – to maximize inbox placement. (We leverage any existing warm-up service the user has, or use an API for automatic warm-up if available).
* **LinkedIn Automation:** The platform will integrate with LinkedIn to send connection requests and direct messages on the user’s behalf (capped to \~15/day to stay safe). Since the official LinkedIn API doesn’t allow sending DMs unless you’re a partner, we’ll use a third-party automation service (like **PhantomBuster** or a similar tool) via its API. The user will provide their LinkedIn session (via a secure token or extension), and our system will trigger automated actions (send invites, send messages) through PhantomBuster’s cloud browser safely. This approach is **usage-based and proven** by other outreach tools, avoiding LinkedIn’s API limitations.
* **AI-Driven Reply Handling:** When leads reply, the AI SDR will categorize and even respond if appropriate. Replies will be automatically fetched: we’ll use the email API (or IMAP) to grab email responses, and for LinkedIn, we can periodically fetch messages via PhantomBuster or prompt the user to log any replies. The system classifies each reply as **positive (interested)**, **neutral (“not now”/not interested)**, or **no-reply**. Leads are tagged accordingly in a **“Leads” dashboard segment**. For positive replies, the AI can draft a suggested follow-up (e.g. to schedule a meeting), which the user can review and send. Neutral replies can trigger AI-generated objection handling responses, helping turn a “maybe” into a meeting. All of this can run in a human-in-the-loop mode: the user can choose to approve AI-written responses or let the AI handle it autonomously (similar to AiSDR’s autopilot vs. copilot modes).
* **Lead Management & CRM Integration:** The platform will display all leads and their status in a simple CRM-like interface. Leads will be grouped by campaign and by status (e.g. “Contacted – no reply”, “Replied – interested”, “Replied – not interested”). Users can click a lead to see the full conversation history across channels and respond directly from the platform (via email or LinkedIn integration). In future, we can sync data back to the user’s CRM (HubSpot, Salesforce) – but for the 30-day MVP, we’ll at least allow CSV export or basic integration to HubSpot via API if time permits.

In essence, the AI SDR platform will **consolidate all prospecting tools in one place** – data sourcing, outreach automation, AI personalization, and communications – giving a *single intuitive interface* to run outbound sales. This “all-in-one” approach is something users love about existing solutions. Our edge will be a **streamlined UI/UX** and tight AI integration for smarter, more human-like outreach conversations.

## Tech Stack and Tools Selection

To meet the aggressive timeline, we’ll use modern, high-level tools that offer **rapid development** and have generous free tiers or pay-per-use pricing:

* **Frontend:** **Next.js + React** – Next.js will allow us to build a fast, SEO-friendly web app with API routes for server logic. React is ideal for dynamic UI, and we’ll use **Saas UI (saas-ui.dev)**, a beautifully designed UI component library (built on Chakra UI) tailored for SaaS apps. This gives us pre-styled components (dashboards, form layouts, tables, modals) to achieve a professional look quickly. We will also use **Tailwind CSS** (if needed for custom styling) or stick with Chakra’s theme for consistency. The UI will be responsive and clean out-of-the-box, reducing our design effort.
* **State Management:** Rely on React hooks and context for state. Next.js pages for routing (dashboard, leads, campaigns, settings, etc.). No heavy state libraries needed in a small app.
* **Authentication & User Management:** **Clerk.dev** will handle sign-up, login, and user profiles securely with minimal coding. Clerk provides pre-built React components for auth flows and supports social logins if needed. It saves us from implementing auth and integrates with Next.js easily. Its free tier should suffice initially (it’s usage-based).
* **Backend & Database:** **Supabase** will serve as our primary backend platform. Supabase offers a hosted Postgres database (with a free tier) and built-in RESTful API/GraphQL for data operations. We’ll use Supabase to store persistent data: user info, leads, campaigns, sequences, and message logs. The advantage is we get instant APIs and can define row-level security policies to ensure data privacy per user. Supabase also has **Edge Functions** (serverless functions in Deno) which we can use for any custom server logic or scheduled jobs (like sending outreach on schedule, or checking for replies). This saves us from deploying our own server. We might also utilize Supabase’s real-time features or webhooks for notifications (e.g. when a reply is received, to update the UI in real-time).
* **External Data APIs:**

  * **ZoomInfo API:** The primary data source for lead search and enrichment. ZoomInfo’s Search API allows querying contacts/companies by filters like title, industry, location, company size, etc., returning relevant prospects. We will integrate this via a serverless function or Next.js API route (to keep the API key secret). ZoomInfo’s Enrich API can fetch full profiles (email, phone, company firmographics) for selected leads. *Pricing:* ZoomInfo API typically requires a contract, but for development we might use a trial or limit calls. Given the time, we might abstract this to easily swap in an alternative if needed (see below).
  * **Apollo.io API:** Backup data provider. Apollo has a large database and a **People Search API** with similar filters, often with a more accessible pricing (Apollo offers free credits for small usage). We can use Apollo if ZoomInfo is unavailable or to supplement results in niche markets (users noted that one source might miss some sectors). Apollo also provides verified emails and even phone numbers for contacts, which is useful. In our design, when the user searches leads, we’ll try ZoomInfo first; if results are insufficient or data fields missing, an Apollo API call can fill the gaps (for example, to get personal emails if ZoomInfo only gave generic or to get extra contacts at a company).
  * **Clearbit API:** We’ll use Clearbit for certain enrichment tasks. Clearbit Enrichment can take a domain or email and return firmographic details (industry, employee count, logo, etc.) which could augment ZoomInfo data. More importantly, **Clearbit Reveal** (or ZoomInfo’s own WebSights) will be used for identifying website visitors. For instance, we can embed a script on the user’s site that captures visitor IPs and use Clearbit Reveal API to convert those to company names and info. These “warm inbound” leads can be shown in a **Web Visitors** section of our app for the user to target (with higher priority since they showed interest by visiting). Clearbit’s usage-based pricing and free tier (100 calls/month) make it suitable for a lean startup.
  * **Google Maps Places API:** For users who need local business leads (e.g. a user targeting restaurants in a city), we integrate Places API to search by category + location. For example, the user could input “spa centers in Chicago” and we fetch a list of businesses, then take their websites from Google and plug those into Clearbit/ZoomInfo to get owner contacts. This is a creative lead-gen angle. The API is pay-per-use with a free credit, so we’ll include it as an *optional* tool within the campaign creation (perhaps under an “Explore local leads” option).
* **AI/NLP Services:**

  * **OpenAI API (GPT-4/GPT-3.5):** The core of our AI capabilities. We’ll use OpenAI’s LLM to perform tasks like: summarizing the user’s website and extracting product value props; researching a prospect (by inputting raw text from their LinkedIn or news snippets) to find personal hooks; generating email and LinkedIn message content that is tailored to the prospect and in the user’s style; and classifying the sentiment/intent of replies. GPT-4 is ideal for quality, but we might use GPT-3.5 for cost-efficiency on some tasks. The OpenAI API is pay-as-you-go, which fits our “usage-based” requirement and allows scaling as needed. We will design prompts for each AI task (e.g. a prompt template for cold email where we fill in {prospect\_role}, {company\_info}, {product\_value}, etc., and let GPT create a short, engaging message). We’ll also implement guardrails: allow the user to review/edit AI content initially (e.g. in a “Copilot” mode) to build trust, with an option to automate fully later.
  * **Possible Future AI Enhancements:** We keep the architecture open to plugging in other AI models or multi-agent systems (e.g. one agent scours the web for news while another drafts the email). But within 30 days, we’ll stick to straightforward LLM API calls orchestrated by simple code – which already gives us powerful “agentic” behavior (the AI can “think” and write content, simulate some decision-making for best times to send, etc.). We will incorporate AI wherever it adds value: even small touches like NLP to parse email replies or an AI to recommend which leads are high priority (based on firmographics or intent signals). These ensure the product feels “AI-first” throughout the user experience.
* **Communication Integrations:**

  * **Email Sending:** We’ll integrate with an email API or SMTP service to send the outreach emails. Since the user has set up sending accounts on a separate domain (warmed up Gmail/Workspace accounts), we can use Gmail’s API (OAuth) to send through those accounts. Another approach: connect via SMTP using credentials for those mailboxes (or use a service like SendGrid if easier, but sending from actual mailboxes yields better deliverability). We’ll create a mechanism for the user to connect their email during onboarding (OAuth consent for Gmail, or input SMTP details if using another provider). Once connected, our backend will use that to send emails in the sequence. We’ll also use the Gmail API to watch for replies (using Gmail webhooks or periodic checks) so we can log responses automatically. This avoids needing the user to BCC or forward emails.
  * **LinkedIn Automation:** As mentioned, **PhantomBuster** (or a similar automation service like TexAu) will be used. The user will likely need to provide a LinkedIn session cookie (PhantomBuster has a browser extension to simplify this). We will store this securely (encrypted in the DB or not store at all, just store PhantomBuster API key + the Phantom ID referencing their LinkedIn session). Using PhantomBuster’s API, we can programmatically: send a connection request with a note, check periodically if it’s accepted, then send a follow-up message, etc. PhantomBuster runs these as cloud jobs mimicking a real browser (to stay within LinkedIn’s anti-bot rules). This approach is proven and “safest” for LinkedIn outreach. PhantomBuster has a free trial and then usage-based pricing (by execution time) which is manageable for a few actions per day.
* **Infrastructure & Deployment:** We will deploy the web app on **Vercel** (optimal for Next.js, with free tier) or an alternative like Netlify. Supabase hosts our DB and functions, so we don’t need to manage servers. We’ll keep environment secrets (API keys for ZoomInfo, Clearbit, OpenAI, PhantomBuster, etc.) in Vercel/Supabase secure store. For image storage (perhaps storing lead avatars or company logos), Supabase Storage or an S3 bucket can be used (though not critical for MVP). The architecture will be **serverless and scalable by design**, relying on third-party APIs and cloud functions.

**Development Tools:** We’ll use **Cursor** (the AI pair-programming editor) to speed up coding. Cursor’s AI assistance can help generate boilerplate and catch bugs rapidly, effectively acting as an extra developer and allowing us to build faster within the 30-day window. We’ll also utilize GitHub for version control and perhaps GitHub Copilot in addition to Cursor for more AI coding help. Using these modern dev tools should significantly compress development time.

Below is the **system architecture diagram** illustrating the platform’s components and integrations:

&#x20;*Architecture Diagram – The AI SDR platform integrates a Next.js frontend, Supabase (DB and backend), and external APIs for data, AI, and communication. Arrows show data flow between components.*

## Architecture & Data Flow

Our architecture follows a **modular, service-integrated design** to maximize re-use of third-party capabilities:

* **Frontend (Next.js + React):** The frontend handles the user interface – dashboards, forms, tables, and the interactive campaign builder. It communicates with our backend either via **Supabase’s client library** (for direct DB reads/writes via API) and via **Next.js API routes** or Supabase Functions for actions requiring secret keys (like calling ZoomInfo or sending emails). The Clerk library will be used on the frontend to manage user sessions (Clerk provides JWTs that we can validate in our backend). The UI will be implemented using SaaS UI components for a polished look without from-scratch CSS. The app will be a single-page application for the main app (after login), with React state managing form inputs and lists for responsiveness.

* **Backend Logic:** We have two primary backends: (1) **Supabase Postgres DB** with auto-generated APIs for basic CRUD, and (2) **Serverless functions** (either Supabase Edge Functions or Next.js API lambda routes) for the heavy logic and external API calls. We’ll write functions for tasks like `searchLeads` (calls ZoomInfo/Apollo), `analyzeWebsite` (calls OpenAI), `sendEmail` (calls Gmail API or SMTP), `sendLinkedInAction` (calls PhantomBuster), and `checkReplies` (checks email/LinkedIn for responses). These will encapsulate the integration details. The backend will also contain the **sequence scheduler**: logic to determine when to execute each step of a campaign. For scheduling, we can use a combination of techniques – e.g. a table of “scheduled actions” in the DB and a CRON job (Supabase Cron or a simple Node cron on Vercel) that runs every few minutes to send due actions. Given the short timeline, a simple periodic poller that looks for actions due (where `send_time <= now() and not done`) and triggers them would work. Supabase Edge Functions can be scheduled (Supabase supports cron triggers) – we’ll use that for simplicity so we don’t maintain a separate server.

* **Database (Supabase Postgres):** The schema will have tables such as: **Users**, **Leads**, **Campaigns**, **CampaignLeads** (join of leads in each campaign with their status), **Sequences/Steps** (defining the outreach steps for a campaign), **Messages** (log of messages sent to leads and replies received), etc. For example, when a user creates a campaign and selects 100 leads, we’ll create a Campaign record, link those leads, and generate sequence steps for each lead with planned timestamps (Day 0 LinkedIn invite, Day 3 email, etc., customized per lead as needed). This way, the scheduler can iterate through pending Messages to send. We’ll also store content like the email templates (or AI prompt inputs) in the DB, and any AI-generated output text (so the user can review what was sent). Using Supabase means we get instant APIs: e.g. the frontend can call `supabase.from('Leads').select(...)` to get lead lists without writing custom endpoints, which speeds development.

* **External Integrations:** As shown in the diagram, the backend will connect to various external services:

  * **ZoomInfo/Apollo/Clearbit:** via REST API calls (HTTPS). We’ll create a data provider module that tries ZoomInfo first, and falls back to others. For example, on a lead search query, the backend function will format the filters into ZoomInfo API call, and if no results or an error (or if the user’s ZoomInfo quota is exhausted), it can call Apollo’s People Search API with equivalent filters. The returned JSON data will be normalized to our Lead schema (fields like name, title, company, email, etc.) before sending to frontend. We will not store all raw data for every search result (to avoid hitting DB limits); instead, store only once the user selects leads to save. Each saved lead will carry an ID/reference to the source (e.g. ZoomInfo contact ID) so we can re-enrich or avoid duplicates.
  * **WebSights/Clearbit Reveal:** To handle website visitor identification, we will provide the user a snippet script. E.g., ZoomInfo’s WebSights script or a custom script that calls our endpoint with the visitor’s IP. Our backend can then call Clearbit’s Reveal API (which takes an IP and returns company info). We’ll design a **Web Visitors** table to log these events (company, visit time, pages, etc.). These can be surfaced in the dashboard as inbound leads for the user to review and add to a campaign with one click. This integration likely requires coordination (the user adding script to their site), so it might be introduced in onboarding or settings.
  * **OpenAI:** We’ll integrate via the OpenAI Python/Node SDK or direct HTTP calls. For example, when analyzing a website, the backend function will fetch the website HTML (perhaps using an HTTP client), extract text (we can use an HTML-to-text utility), then prompt GPT: *“Summarize what \[Company] offers and the value it provides in 3-4 bullet points.”* Similarly, for drafting emails: *“Compose a cold outreach email to \[LeadName], a \[LeadTitle] at \[Company], about how \[Product] can help solve \[pain point]. Include a personalized intro referring to \[personalization insights]. Keep it under 150 words.”* These calls will be made server-side to keep the API key hidden and to handle the async nature. We’ll log the outputs in the DB so the user can see/edit before sending.
  * **Email (Gmail API / SMTP):** We’ll likely use Node libraries (like Nodemailer) or Google’s API client to send emails. If using Gmail API, after user OAuth, we get a refresh token stored in DB, which our backend uses to send emails on that user’s behalf via Gmail REST API. Alternatively, for SMTP, we store the SMTP creds (or an App Password for Gmail) encrypted and use Nodemailer to send. The sending will happen either directly when triggered by the schedule or via a function call at the right time. For receiving emails, Gmail API’s watch endpoints can send a push notification to a webhook when new mail arrives; if time permits, we’ll implement that so replies trigger our endpoint in near real-time. Otherwise, a scheduled job to check the mailbox for new replies (e.g. every hour) can suffice initially. We’ll parse reply content (strip quoted text, etc.) and update the lead’s status accordingly.
  * **LinkedIn via PhantomBuster:** We will use PhantomBuster’s REST API. The workflow: the user logs into PhantomBuster (free trial or their account) and connects their LinkedIn via PB’s extension, generating a **session cookie** that PhantomBuster can use. In our app, the user provides the PhantomBuster API key and the LinkedIn session cookie (or we walk them through setting up a PhantomBuster “agent” for LinkedIn messaging). We then trigger specific PhantomBuster “Phantoms” – e.g., “LinkedIn Network Booster” to send connection requests, or a custom API Phantom to send a message. PhantomBuster allows passing a list of targets (profile URLs or IDs) and a message template. We’ll feed in our lead list (with their LinkedIn URLs from ZoomInfo if available, or we can search LinkedIn by name + company via PhantomBuster’s search Phantom). The sequence logic will decide when to invoke these. After an action, we might query PhantomBuster’s result (they often produce a CSV of completed actions) to confirm success. For accepted connection detection: we can use a Phantom that checks your connections for new ones or simply wait until it’s time to send a LinkedIn message and attempt it (PhantomBuster will only succeed if connected). While not perfectly real-time, this covers the flow. Because LinkedIn doesn’t provide direct reply data, we may not automatically get the content of a LinkedIn reply. Instead, we’ll mark as “Replied on LinkedIn” if the lead sends any message (PhantomBuster has a “LinkedIn Inbox Scanner” Phantom that can scrape unread messages). We can run that daily to pull any new messages from prospects and then use AI to classify them. This part is a bit experimental but feasible.
  * **Others:** If needed, integrate an **email verification API** (like NeverBounce or ZoomInfo’s verify) to double-check deliverability of emails before sending, ensuring we maintain a good sender reputation. Also, any *intent signals* (like if ZoomInfo provides buyer intent or tech stack filters) can be integrated to prioritize leads; for instance, ZoomInfo can filter companies by “recently raised funding” or “hiring for X roles” which imply need – we’ll expose those filters if available in the API.

* **Security & Compliance:** All sensitive data (API keys, LinkedIn cookies, SMTP passwords) will be stored securely (Supabase has an encrypted secrets store for functions; for the DB, use encryption or at least restrict access). We’ll enforce that each user’s data is isolated via Supabase RLS policies (match `user_id` on records). We’ll also include compliance measures – e.g. easy suppression list import to avoid emailing people who opted-out (this could be a simple feature where user uploads emails to never contact, and we’ll exclude those in searches). The system will also include an **unsubscribe link** in emails and handle removing leads who unsubscribe (and informing ZoomInfo via Compliance API if needed to honor privacy laws). These details ensure our platform is responsible and not seen as spam automation.

In summary, the architecture ties together a **React frontend, a serverless backend connected to a Postgres DB, and numerous external services (for data, AI, and messaging)** into one seamless system. We chose high-level services (Supabase, Clerk, etc.) to avoid reinventing the wheel, allowing focus on the unique AI SDR logic.

## Product Design – UI/UX and Pages

We aim to deliver a **highly intuitive and modern interface**. The UI will be clean, minimalistic but visually appealing – leveraging Saas UI’s elegant design and ensuring consistency in spacing, colors, and typography. We will improve upon Artisan’s UI by simplifying complex setup flows and adding helpful AI assistance throughout (making the app feel like a guided experience rather than a complex tool). Below is a breakdown of the key pages/screens and their functionality, along with how they connect to the backend and tools:

### 1. Onboarding Wizard

Upon the user’s first login, they will go through a guided onboarding (step-by-step modal or dedicated page sequence). This fosters quick setup:

* **Step 1: Company Info & Website Analysis** – We ask the user for their **company name, website URL, and product/service description or “value proposition”**. (If they have marketing copy handy, they can paste it, or we let AI extract it from the website.) When the user enters the URL, we trigger our backend to fetch and analyze the site. We might show a loading animation (“Analyzing your website with AI…”) and then display **AI-generated bullet points** of what their product does and the value it provides. The user can confirm or edit these points (this ensures the AI’s understanding is correct). These confirmed points will be saved and later inserted into outreach messages for personalization. *Tech:* This uses OpenAI under the hood.
* **Step 2: Ideal Customer Profile (ICP)** – We prompt the user in simple terms: *“Who is your target customer? (e.g., ‘IT managers at fintech companies with 50-500 employees in US’)”*. The user can type a description, and we also provide structured fields: **Industry**, **Target Titles/Roles**, **Company Size**, **Geography**, etc. (This maps to filters for lead search.) We will use a combination of free text and dropdowns for common options. Perhaps we even allow an AI suggestion here: if the user enters an industry, the system could suggest related roles or sub-industries. This info will pre-fill the search filters in the campaign creation later.
* **Step 3: Connect Email & LinkedIn** – The wizard will then guide connecting channels:

  * *Email:* The user clicks “Connect Email” which initiates either an OAuth flow (for Gmail, open a popup to Google OAuth) or instructs them to input SMTP details if not Gmail. We’ll verify the connection by sending a test email or using Gmail API’s test endpoint. Once connected, we show a success state (e.g., a checkmark “Email connected: [john@outbound.yourcompany.com](mailto:john@outbound.yourcompany.com)”).
  * *LinkedIn:* We explain how to connect LinkedIn (likely by connecting PhantomBuster). Possibly: “Install the PhantomBuster browser extension and provide your API key”. Alternatively, we generate a unique link for them to follow that sets up PhantomBuster with their LinkedIn cookie (PhantomBuster supports connecting via their website too). This part might involve more user effort, so we’ll provide clear instructions with screenshots. If the user isn’t ready, they can skip and do it later in Settings – but we highlight that without LinkedIn connection, LinkedIn steps won’t run.
* **Step 4: Warmup Domain & Sending Settings** – If not already done externally, we might include a reminder: “We recommend using a secondary domain for cold email. If you haven’t set one up, do that first.” Assuming the user already did (as they said), we ensure we have their sending domain info. We can offer them to enable our automated warm-up service if needed (since they have it, we likely just acknowledge it). Also, we ask for the **daily limits** they want to adhere to (e.g., “Max 15 LinkedIn invites/day, Max 50 emails/day”) so our sequence scheduler can throttle accordingly.
* **Step 5: Tutorial or AI SDR Demo (optional)** – Possibly, we welcome them with an example: “Your AI SDR is ready! She will now find and engage prospects for you. Next, create your first campaign.” We might show a sample prospect and a sample AI-generated email for illustration, to build user confidence. This step is skippable.

*UX Note:* The onboarding uses friendly, non-technical language (e.g., “Tell us about your product” vs. “Enter value proposition”) and gives tooltips explaining why we ask each item (e.g., hover info: “We use your website to help the AI understand your product so it can write better outreach emails.”). The design will use progress indicators (step 1 of 4, etc.), and maybe some illustrations or iconography to make it engaging. By the end, the key integrations and data we need are in place.

### 2. Main Dashboard

Once onboarded, the user lands on the **dashboard**. This page provides a high-level overview and quick access to core actions:

* **Key Metrics Cards:** At the top, show summary stats like *Active Campaigns*, *Total Leads Contacted*, *Response Rate*, *Meetings Booked*. For MVP, even a simple count of “Leads approached: X, Replies: Y (Positive: Z)” is useful. If possible, display the positive/neutral/negative counts or percentages. These KPIs give the user immediate feedback on the AI SDR’s performance (and can be empty initially with a “0 – let’s start your first campaign!”).
* **Pipeline Segments (Leads Status):** A section showing three columns or tabs – **Positive**, **Replied (Neutral)**, **No Response** – representing lead segments. Each segment might list the top 5 recent leads in that category with brief info (name, company, last activity). For example, under Positive, list who replied positively and perhaps a one-click button “Schedule meeting” or “Respond”. Under No Response, maybe “Leads with no reply after sequence – consider a new approach or mark as cold.” This segmentation aligns with the user’s requirement of grouping leads by response outcome. It helps the user quickly focus on hot leads. We will highlight the positive ones (maybe with a green accent) to draw attention.
* **Active Campaigns List:** A table or list of current campaigns with summary info: *Campaign Name*, *# Leads*, *Status (Running/Completed)*, *Open Rate*, *Reply Rate*, etc. Each entry can have an action to view details or pause/stop the campaign. This gives the user multi-campaign management capability. (If the user only runs one campaign at a time initially, this is still useful to list history.)
* **Call-to-Action for New Campaign:** A prominent button “🚀 Create New Campaign” to guide the user to launch the next outreach. This likely stands out (perhaps top-right or a big empty-state illustration if no campaigns exist yet).
* **Tips/Insights (optional):** We can dedicate a small area for AI-driven tips, e.g., “💡 Pro Tip: Tuesday 10am has the highest email reply rates. Consider scheduling emails at that time.” or “Your open rate is 60%, which is above industry benchmark 🎉.” These can be simple static tips at first, and later personalized using our data.

The Dashboard page is mostly read-only analytics and navigation. It pulls data from the DB (campaign stats, lead stats) – those can be computed on the fly via SQL (Supabase can do COUNT with filters easily) or pre-calculated as part of the sequence processing (e.g., update counters whenever a reply comes). For MVP, computing on load is fine. The UI for the dashboard will use cards and grid layouts from SaaS UI, ensuring it looks professional (e.g., similar to a CRM or marketing automation dashboard).

### 3. Lead Search & Campaign Creation

This is the heart of the application – where the user defines who to contact and launches the AI SDR to do so. We will likely implement this as a **multi-step wizard** (to simplify the user experience instead of one giant form). Key sub-pages or components:

**(a) Campaign Setup Form:** The user first enters high-level info for the campaign:

* *Campaign Name:* e.g. “July Fintech Outreach” (for their reference).

* *Target Definition:* We reuse the ICP info – possibly pre-filled from onboarding. The user can adjust or refine filters for this campaign. We provide UI controls for the most useful filters:

  * **Persona/Title:** e.g. CEO, Marketing Manager, or multiple titles. (A multi-select dropdown with common titles, and a free-text add option.)
  * **Industry:** dropdown of standard industries (or multi-select).
  * **Company Size:** perhaps a range slider or tiered options (1-10, 11-50, 51-200, etc.).
  * **Location:** text or dropdown for country/state/city. We might integrate a location autocomplete API for convenience.
  * **Keywords (optional):** any keyword to match in company descriptions or titles (could feed to ZoomInfo’s keyword filter).
  * **Data Source toggle:** If we offer multiple sources, we could allow advanced users to pick “Use ZoomInfo” or “Use Apollo” or “Both”. But likely we hide this complexity and just default to our combined pipeline internally.
  * **Lead Type:** “People” vs “Companies”. If the user wants to first search companies then specific people in them, we could allow searching companies by criteria (industry, size, etc.), then pick certain companies and find contacts at those. For speed, however, we may directly search contacts by role (which implicitly filters company attributes too). We’ll probably go with **People search** (so the results are individuals with emails).

* *Advanced Filters (optional accordion):* e.g. “technologies used” (ZoomInfo can filter by tech installed, like show companies using AWS or using Salesforce, etc.), “recent funding” or “hiring” signals. These are more advanced and can be added if time permits or left for future.

The UI for filters will be built with form components. SaaS UI likely has form layouts we can use. We’ll validate inputs (e.g., require at least one of title or industry etc. to avoid super broad queries).

**(b) Lead Search Results:** After the user enters filters and hits “Search”, we call our backend (ZoomInfo API). We’ll display a **loading spinner** and then show a paginated list of results: likely a table with columns like *Name*, *Title*, *Company*, *Location*, maybe *Industry*. We might also show an icon if email/phone is available. The user can scroll or page through and select the leads they find relevant. We’ll include a “Select All” (for the current page or all results) to easily add many leads. On the side or top, show a count of selected leads.

Given ZoomInfo could return hundreds or thousands of results, we might limit to the top N (say 1000) or encourage refining filters. The user can narrow down by adjusting filters if too many results; we could show the total count found (“5000 contacts found”) and if it’s huge, suggest adding more criteria.

Each result row might have a “Preview” action – e.g., hover to see more details about that contact (like a mini card showing email, maybe a LinkedIn URL or short bio). To get those details we might call ZoomInfo Enrich for that contact ID on demand. This is similar to how sales tools let you preview a lead’s info before deciding.

The selection of leads might be final with just checkboxes, or we might have a two-pane flow: where selecting and clicking “Next” goes to a **Selected Leads review** screen. In that screen, we list the chosen leads and allow the user to remove any and then confirm. We’ll also show how many credits they might be using (if data cost is a thing – e.g., “You are about to export 50 contacts”). But since it’s our internal usage of API, the user doesn’t directly pay per lead in our app for MVP; still, transparency can be good.

**(c) Enrichment & Confirmation:** Once leads are selected, we click “Next: Enrich Leads”. The system will then fetch additional info for these leads (if not already in results). For example, ZoomInfo’s initial search might give name/title/company, but we may need to call Enrich to get email and LinkedIn URL. We will perform that in bulk (maybe sequentially if no bulk API, or a few parallel threads). During this step, we can show a progress bar like “Enriching 50 leads…” to keep user engaged. If some leads lack data or fail, we note that (or replace from Apollo if we have that fallback logic auto).

After enrichment, we display the **final list of leads** with the key contact info that will be used:

* Email addresses (so user knows we can reach them by email). If some are missing, we might flag “no email found – will skip email step for those” or try an alternate source (Apollo often can fill email).
* LinkedIn profile links (helpful to have, for LinkedIn outreach).
* Any “personalization insights” our AI found (we could at this point run our “digital footprint analysis” for each lead – but doing that for 50 leads could be slow. Instead, we might do it on-the-fly later when composing each message). More likely, we prepare a “context” for each lead like company description and maybe one recent news headline by calling an API or scraping company’s news. If time allows, at least for each lead’s company we could fetch one recent news item via a news API for use in email. But this could be an advanced addition; MVP may skip it and rely on generic personalization (like mentioning their role or industry trend).

User can scroll this list to double-check. If everything looks good, they proceed to **compose the outreach sequence**.

**(d) Sequence Configuration:** This page lets the user define *how* the campaign will reach out and the content to use. Key elements:

* **Sequence Template Selection:** Offer a few preset outreach flows, e.g.:

  * *“LinkedIn First”* – Step 1 LinkedIn invite, Step 2 LinkedIn message (if connected), Step 3 Email (after X days if no reply).
  * *“Email First”* – Step 1 Email, Step 2 Email follow-up, Step 3 LinkedIn connect, etc.
  * *“Email Only – 3 Touches”* – Step 1 Email, Step 2 Follow-up email, Step 3 Final email.
    Users pick one or we allow fully custom sequence builder (drag-drop). Given time constraints, we likely provide 2-3 common patterns to choose from via radio buttons or a small diagram illustrating each. Once chosen, a summary of steps is shown (with default timings). We allow tweaking delays (like “wait 3 days before email follow-up” etc.). The UI might be a simple list of steps where they can adjust a number of days or turn a step on/off. For example: “Day 0: Send LinkedIn Connection \[x]”, “Day 2: If not connected, send Email 1 \[x]”, etc., with checkboxes or inputs for days. We will set sensible defaults based on best practices.

* **Email Content & AI Settings:** For each type of message in the sequence (initial email, follow-up email, LinkedIn message), we let the user either (a) **allow AI to generate from scratch** or (b) provide a template or guidelines. Because this is an “AI-first” tool, we will heavily emphasize that the AI can handle writing – we might simply ask for *“Tone”* and *“Key points to mention”* and then have the AI draft the actual text when sending. For example:

  * *Tone:* a dropdown or buttons for “Formal / Professional”, “Friendly / Casual”, “Funny”, etc. (This will adjust the prompt style or choose which playbook to use.)
  * *Key Product Hook:* (pre-filled from the website analysis, i.e., the main value proposition). The user can edit or add more context here if desired like “Emphasize how our solution saves 30% cost” or “Mention our recent \$5M funding for credibility”.
  * *Call-to-Action:* Perhaps a choice like “Book a meeting link” vs “Ask a question” vs “Invite to reply”. This influences the email’s closing line.
  * *Variables:* Show which personalization tokens the AI will include (like {LeadName}, {CompanyName}, etc.) so the user knows it will be personalized.
    We could also show an example output (“Preview email”) after the user inputs these settings – calling OpenAI once to generate a sample for one lead. The user can regenerate if not satisfied or modify the hook/tone. This gives confidence in the AI content.

  For advanced users, we might allow them to input their own starting template with placeholders and let AI fill in the blanks where needed. But given the time, a simpler approach is just capturing a few preferences and doing it all AI.

* **LinkedIn Message Content:** Similar to email, but typically shorter. We may just use the same AI logic to generate a concise connection note and later a follow-up message. We’ll inform the user if LinkedIn steps are used, the AI will auto-write those contextually (and they’ll be able to see them in logs). If they want, they can provide a one-liner to include.

* **Approval Settings:** A toggle for “Require approval before sending each message”. If ON, the campaign will generate drafts and queue them, but notify the user to approve in the app before actually sending. If OFF (autopilot), it will send on schedule automatically. We anticipate most will start with approval to trust the AI, then maybe later fully automate. So we’ll default to ON for initial campaigns and let them turn it off explicitly.

Once sequence and content settings are reviewed, the user hits **“Launch Campaign”**. We show a confirmation dialog “Campaign XYZ will contact 50 leads over the next 2 weeks via \[channels]. Confirm?” When confirmed, we create the campaign in the DB, save all parameters, and kick off the scheduling logic. The user is then taken to the **Campaign Details page**.

### 4. Campaign Details & Timeline

This page appears right after launching a campaign (and is accessible by clicking a campaign from dashboard). It provides a detailed view of that campaign:

* **Campaign Overview:** Name, start date, how many leads, which sequence template. Possibly a visual timeline of the sequence (like a Gantt-style line or just text: “Day 0: 50 LinkedIn invites, Day 2: Email to remaining leads, …”). Could show stats like how many invites sent, how many emails sent, how many replies received so far. If campaign is ongoing, stats update as it runs.
* **Leads Table (Campaign-specific):** List of all leads in this campaign with columns: Name, Company, Status, Last Activity, etc. Status could be “Pending LinkedIn invite”, “Invite Sent”, “Connected”, “Email Sent”, “Replied – Positive/Neutral”, etc. We’ll update these in real-time as actions happen. The user can filter this table by status (show only “Needs attention” which could be replied leads).
* **Controls:** Options to **Pause** or **Resume** the campaign. Pausing would stop new scheduled actions (but not unsend already sent ones, obviously). This is useful if the user wants to halt sending (maybe they got enough responses or need to edit something). Also possibly an option to add more leads to campaign if they want (which could open the search step again but adding to existing campaign).

This page is mostly for monitoring and manual intervention. If approval mode is on, this is where the user would see pending messages. E.g., “5 emails are ready to send, waiting for your approval” with a list; the user can click each to view the AI-generated content and hit Approve or Edit. We will make that editing possible inline (a modal with the draft and an edit box).

The campaign detail view keeps the user informed of what the AI SDR is doing, building trust.

### 5. Leads/Inbox (Conversation Management)

To handle replies and ongoing conversations, we’ll have an **“Inbox” or “Leads” page** that functions like a mini-CRM inbox:

* This page might show a list of all leads who have replied, with the most recent replies on top (like an email inbox). Each entry shows lead name, company, a snippet of their reply, and maybe an icon whether it was via Email or LinkedIn. Unread replies could be highlighted. The user can click a lead here to open the conversation thread (or mark it as handled).
* Alternatively, we implement this as part of the Leads table by enabling a detail drawer: e.g., clicking a lead anywhere (on dashboard segments or campaign leads table) opens a side panel or separate page with that lead’s **full conversation**. For simplicity, a dedicated “Conversations” page might be clearer.

**Conversation View (Lead Detail):** Here we display the history of interactions with that lead:

* Outbound messages sent: show the content of the LinkedIn invite, LinkedIn message, or email that was sent (we can label them with channel and date). Since AI generated them, the user might be seeing the exact content possibly for the first time (if they didn’t require prior approval). So this transparency is important.

* Inbound replies: show the lead’s reply content, with perhaps a tag if it was classified positive/neutral. We could even highlight key sentences (using sentiment analysis, but maybe later).

* Provide a **reply box** for the user to respond. If the last inbound was an email, replying here sends an email back (via our email integration). If it was LinkedIn, replying here triggers a LinkedIn message via PhantomBuster. We’ll need to indicate that (“This will send via LinkedIn account X”). If we can’t send automatically (some complexity with LinkedIn?), we might just copy the text to clipboard or prompt the user to send manually. But assuming PhantomBuster can do it, we handle it similarly to initial messages.

* **AI Assist for Replies:** Next to the reply box, have a button “✍️ AI Suggest Reply”. Clicking it will call OpenAI with the conversation context (especially the lead’s last message) and generate a suggested response. For example, if the lead said “Can you send me pricing info?”, the AI might draft a polite response with a short answer and an invitation to discuss more. The user can edit it as needed and hit send. This feature saves time and maintains the AI support through the whole process. It essentially extends the AI SDR agent to assist in **2-way communication**, not just initial outreach (though fully automated handling of conversations may be a future feature, we at least provide suggestions now).

* **Lead Info Panel:** On the conversation view, also show the lead’s profile info at a glance (maybe on a sidebar or header): title, company, LinkedIn link, etc., and any notes. We can also include the personalization insights we gathered (e.g., “Talking Points: Mention their recent funding; note: they tweeted about needing better HR software” – if we had such data). This helps the user craft responses if they choose to do it themselves.

This page essentially combines the function of an email client and LinkedIn messenger, tailored to sales conversations. The UI will use a chat-like format (messages bubbles or blocks). SaaS UI may not have a chat component, but Chakra UI easily styles message lists. We’ll keep the design simple: left-aligned lead messages, right-aligned user/AI messages, different colors.

### 6. Settings & Integrations

A settings page will allow the user to manage connections and preferences:

* **Account Settings:** Basic profile info, password, etc., though Clerk handles most of that. Possibly an option to invite team members if multi-user (not in MVP scope if single-user).
* **Email Integration:** Show connected email accounts. Option to connect a new one or re-auth. If connected, show status (e.g. “Gmail account X connected. 100 emails/day limit.”). If using SMTP, allow updating credentials. Also show domain health if available (maybe integration with the warm-up service’s API to fetch stats: e.g., “Domain reputation: Good, 0% spam rate” if accessible).
* **LinkedIn Integration:** Show if LinkedIn (PhantomBuster) is connected. If not, present the steps to do so (with links and maybe an input for session cookie). Possibly test the connection by retrieving the user’s profile.
* **API Keys:** If our users want to plug in their own ZoomInfo or OpenAI API keys (to use their credits), we could allow that here. But that might complicate things; likely we handle all under the hood. However, giving an option for OpenAI key could offload cost to user if needed. For now, probably not needed since usage is moderate and our account can handle it.
* **Preferences:** e.g., toggle default autopilot vs approval, timezone settings (important for scheduling emails at local times), working hours (to avoid sending at 2am), etc. We default to reasonable ones but let user change.
* **Team & Billing:** If this were a SaaS product, we’d have plan and billing info here. In a 30-day build, we might skip payment integration and just operate on a free trial mode. We can include a placeholder for “Usage: X leads used out of Y free” etc., if desired.

The settings page ensures the user can update connections or limits after onboarding. It will be built mostly with form elements and use the Clerk/Supabase APIs to update info.

### 7. Design and User Experience Considerations

Throughout all pages, our focus is on **clarity, guidance, and aesthetics**:

* Use a **consistent layout**: likely a sidebar navigation (with items Dashboard, Campaigns, Leads, Settings) and a top bar for quick actions or user menu. SaaS UI has prebuilt layouts we can adapt (ensuring mobile responsiveness as well).
* **Visual hierarchy:** Important info like lead names, stats, and AI suggestions will be visually prominent. Less important settings or advanced filters will be tucked away until needed.
* **Feedback and loading states:** Because many actions involve external APIs (which may take a couple seconds), the UI will always indicate loading (spinners on buttons or skeleton screens) to avoid confusion. For long-running processes (like enriching many leads or waiting for AI generation), we’ll use modal progress bars or notifications (“Generating emails for 50 leads, this may take \~30 seconds…”). Possibly perform some tasks asynchronously and notify when done (we could integrate Supabase’s Realtime or just periodic refresh to update status).
* **Error handling:** If an API fails (ZoomInfo downtime or OpenAI error), we’ll show user-friendly messages and allow retry. For example, if lead search fails, “We had trouble fetching leads. Please check your API credits or try again later.” Also log these errors for us to debug.
* **Aesthetics:** Use a modern color palette – likely a neutral light theme with an accent color (perhaps blue or teal, as those convey trust and match many SaaS). Ensure sufficient contrast for readability. Use icons (e.g., material icons or Feather icons) next to sections (like a 📧 icon for email steps, a LinkedIn logo for those steps) to make it visually clear.
* **Better than Artisan’s UI:** From user feedback, Artisan’s interface could use modernization. We will make ours more streamlined: fewer clicks to get things done, more intuitive labeling, and helpful defaults. For example, Artisan might require filling a playbook; we instead let the AI handle it with minimal input, reducing form fatigue. Our UI will also be responsive and accessible (proper alt text, keyboard navigation) which adds to UX quality. We aim for a “sleek experience built for outbound” as highlighted in competitor comparisons.

### Wireframes & Mockups

Before coding, we will create low-fidelity wireframes for each of the above pages in Figma. This helps finalize the layout and flow. Then we’ll progress to high-fidelity mockups using Figma’s design features, applying our chosen color scheme and component style (likely leveraging Chakra UI’s design tokens for consistency). These mockups will guide development and ensure we have a clear visual target. Each page’s design will align with the modern SaaS dashboard style – plenty of whitespace, clear section headings, and tooltips or info icons to explain AI features (education is key since AI decisions might not be obvious to the user).

Given time constraints, we might not be able to polish every visual detail, but using Saas UI’s components means we start with a polished baseline (professional fonts, spacing, etc.). We will focus extra effort on the critical screens (lead selection and conversation UI) to make them intuitive. For example, the conversation view might resemble an email thread interface with clear separation of AI vs human messages – ensuring the user trusts the AI isn’t going off-script.

*(Note: Actual Figma files would ideally be included in a complete design hand-off, but for this research plan, we describe their contents.)*

## Implementation Timeline (30-Day Plan)

To build this in 15–30 days solo, we will follow an **aggressive but structured schedule**. The plan is to create a functional MVP by day \~20, then use remaining days for testing, UI polishing, and integrating any stretch features if possible. We’ll iterate quickly, leveraging AI coding assistance (Cursor, Copilot) to generate boilerplate and focusing on critical logic. Here’s a breakdown by week (assuming \~4 weeks = 30 days):

* **Week 1: Planning, Design, and Setup (Days 1-7)**

  * *Day 1-2:* Finalize requirements and architecture. Prepare the ERD (entity-relationship diagram) for the database and ensure it covers all data we need (Leads, Campaigns, etc.). Set up the development environment: create the Next.js project, initialize Supabase project and database, integrate Clerk for auth (test user signup flow). Verify that a user can log in and we can get a JWT in Next API routes.
  * *Day 3:* Design wireframes for all main screens (Dashboard, Campaign wizard steps, Leads inbox, Settings). Iterate quickly in Figma – get feedback (if any peer review available) on flow. Begin applying Saas UI theme to see how components will look.
  * *Day 4-5:* Implement the basic UI skeleton: Navigation layout with sidebar and dummy pages for each section (just to navigate). Implement the onboarding wizard UI based on the wireframe (without actual functionality yet). Also, code the forms for campaign creation (filters UI) and a static table for lead results (with dummy data) to flesh out the layout. This can be mostly front-end work with placeholder data.
  * *Day 6:* Integrate Supabase DB schema – define tables and test inserting a sample campaign, lead, etc. Write a simple API route or function to create a campaign and ensure data flow from form to DB works (dummy data for now). If time, start integration with ZoomInfo API: get API keys ready, make a test call from a Node script to ensure connectivity. Also, attempt a test call to OpenAI API from Node to confirm env setup for calling external APIs.
  * *Day 7:* Finish any remaining UI elements of onboarding and campaign wizard. Implement the website analysis step: write the function to fetch site content and call OpenAI, and connect it to the onboarding UI (so it displays the summary bullets on step 1). Test with a real website or two. By end of Week 1, we should have a **basic app shell** with navigation, user can login, go through onboarding (with AI analysis working), and possibly see a stubbed campaign creation page.

* **Week 2: Core Features – Lead Sourcing & Outreach Setup (Days 8-14)**

  * *Day 8:* Work on the **lead search integration**. Implement the `searchLeads` API route: take filters from the frontend, call ZoomInfo’s Search API. Parse results, return to frontend. On the UI, display real results in the table. Ensure selection works (store selected leads in React state or context). If ZoomInfo API is not readily available (due to key access), integrate Apollo’s People Search API as a backup for development/testing. By end of day, user should be able to input filters and get actual lead data listed.
  * *Day 9:* Implement **lead enrichment** after selection. For each selected lead, call ZoomInfo Enrich (or Apollo enrichment) to get full details. Use `Promise.all` with a throttle to handle multiple calls. Save the enriched leads to the database (Lead table) with the user’s ID and campaign ID (if campaign already created by then). This is when we actually create the Campaign and Lead records in DB. Ensure that we avoid duplicate leads (maybe use unique email as a key) – but that might be fine to ignore in MVP.
  * *Day 10:* Connect the **campaign sequence configuration** UI. Implement logic for the sequence presets (could store these presets in a config object). When user chooses a preset and sets delays, on “Launch”, create the Sequence steps in the DB for each lead. Also, generate the initial messages via OpenAI for each lead now vs later? We have two approaches:

    1. **Generate all messages upfront:** easier to implement (just loop through leads and call OpenAI for each message needed, store them). But this could mean 50+ OpenAI calls in one go, potentially slow (we might queue them).
    2. **Generate just-in-time:** schedule the sending and only call OpenAI right before sending each message. This is efficient in spreading load and having up-to-date context (especially if a lead’s situation changes or more info later). However, just-in-time requires the scheduler to call AI service at send time.
       We might do a hybrid: generate the first touch messages immediately (so the user can review them if approval is on), and generate follow-ups later if needed. Implement the code for one of these approaches. Also, integrate the “approval required” setting: if on, mark the messages as pending approval in DB and don’t actually schedule send until approved.
  * *Day 11:* **Email integration:** Set up the email sending service. If using Gmail API, do the OAuth flow (perhaps using NextAuth or directly hitting Google’s endpoints). Test sending a sample email through the connected account. If using SMTP, test with nodemailer using a test Gmail account credentials. Once one approach works, integrate it such that the campaign scheduler or send function can call `sendEmail(to, subject, body, userCredentials)`. Also, create a template for email content that includes tracking variables (e.g., add an unsubscribe link and maybe a tracking pixel image if we want to track opens – though that might be too advanced; skip pixel for now).
  * *Day 12:* **LinkedIn integration:** Sign up for PhantomBuster, get an API key, and try out a phantom manually to see how it works (this might be outside coding, more of configuring PhantomBuster and reading their docs). In code, write a helper function to trigger a PhantomBuster call. For example, use their **LinkedIn Network Booster** Phantom: you send it a list of LinkedIn profile URLs and a message, it sends invites. We need to figure out how to supply input: likely we need to create a CSV or send JSON via their API. Implement a simple version: for now, perhaps just log the URLs that should be sent and assume success, we will integrate fully in Week 3. (If time allows, actually call the PhantomBuster API and verify it runs and returns a result.)
  * *Day 13:* **Sequence scheduling logic:** Build a function that picks up scheduled messages and sends them. Simplest approach: write a cron (Supabase function or even a setInterval in Next.js if that runs, but better a backend job) that every minute checks for any sequence steps due in the past minute. For each, if not sent, perform the send action (if email, call sendEmail; if LinkedIn, call PhantomBuster send; if it requires waiting for accept, handle logic). Mark them as sent in DB and log the timestamp. This might also create the next step if it depends on a condition (like after a LinkedIn accept, schedule the LinkedIn message). But for MVP, we might schedule all in advance with conditions and then at runtime skip if condition unmet (e.g., if step says “send LinkedIn Message if connected = true”; we check DB flag “is\_connected” for that lead updated by accept event). For now, we can assume a simpler model: always send both channels with a delay (since the worst is they get an email even if they accepted on LinkedIn – not too bad for MVP). We will refine condition handling if time. Test the scheduler with a small test: schedule an email 1 minute in the future, see that it sends.
  * *Day 14:* **UI polish for campaign flow:** Now that search and sequence logic are in place, test the entire campaign creation in the UI: enter filters, get leads, select a few, go through sequence config, launch campaign. See that records are created, and maybe fast-forward (set short delays) to see emails actually send to a test inbox. Fix any bugs encountered (likely plenty of integration issues). Work on edge cases: what if no leads found (show a message “No leads match, try broadening filters”), or if API returns error (handle gracefully). Also ensure that the app is still secure: use Clerk user ID to label data, ensure one user can’t see another’s leads (Supabase RLS to the rescue). By end of Week 2, the **campaign creation and execution pipeline should be functional** end-to-end in a basic form.

* **Week 3: AI Features, Reply Handling, and Refinements (Days 15-21)**

  * *Day 15:* **AI content enhancements:** Improve the prompt engineering for OpenAI to get better email copy. Incorporate the website analysis output and any personalization insights into the prompt. For example, build a function `generateEmailLeadMessage(lead, campaign)` that prepares a detailed prompt and calls OpenAI. Test with a few sample leads to ensure the tone and content are good. If outputs seem generic, tweak prompts (maybe include more specifics like prospect’s company achievement or pain point). Also implement the **AI LinkedIn message** generation (slightly different prompt for brevity and casual tone). If time, implement multi-variant testing: perhaps generate 2 versions of the first email and randomly assign leads version A or B to see which performs better (this is a stretch goal mentioned by Artisan, but if quick to do, it could be a nice differentiator). It might be as simple as an option “Enable A/B test – generate two variants and split leads”.
  * *Day 16:* **Reply capture (Email):** Implement a mechanism to get inbound email replies. E.g., use Gmail API’s `messages.list` or watch for new messages in a label. For simplicity, set up a cron job to check the mailbox every 10 minutes for any new messages from the leads’ email addresses (we have those stored). Gmail API can query by sender or thread id. Alternatively, when we send an email, we can include a unique identifier in the email (like in Reply-To address or in the message ID) to easily identify replies. Perhaps simplest: use the sending email’s inbox and search for any new messages that are replies to our sent ones. Implement parsing logic: extract the body text of the reply and ignore quoted original. Save the reply content to Messages table and mark lead as Replied. Also run the classification: call OpenAI or a simple heuristic to tag it positive/neutral. Positive if contains words like “interested, schedule, yes”, neutral if “not now, maybe later, not interested”, negative if no reply or explicit “unsubscribe” (also handle unsubscribe requests automatically by flagging that lead and not contacting again).
  * *Day 17:* **Reply capture (LinkedIn):** If using PhantomBuster’s “Inbox scanner”, integrate it now. Possibly schedule it to run every hour and retrieve messages. Parse the output (PhantomBuster usually gives a JSON or CSV of messages with sender, content). Filter those to find any from our leads (we can match by lead name or if we stored LinkedIn profile IDs). Save those as replies in DB as well. If implementing this is too complex or unreliable, an interim approach: when a LinkedIn connection is accepted, mark that, and if they reply in LinkedIn, maybe prompt the user to check LinkedIn (since fully capturing LinkedIn replies might be version 2 feature). But we aim to at least capture acceptances (PhantomBuster can notify on acceptances easily). So at minimum: update lead status to “Connected” when they accept, which we treat similar to a neutral reply for follow-up logic.
  * *Day 18:* **Leads Dashboard & Segmentation UI:** Now focus on the Leads/Inbox page in the UI. Implement the table or list showing leads by segment (positive/neutral/none). The data for this can be computed: e.g., a simple filter where `status = 'positive'` etc. Use Supabase query or bring the data to frontend and filter in state. Ensure that this page updates after replies come in (we might use a polling or simply refresh for demo). Implement the conversation detail UI: show message history. Populate it from Messages table records for that lead. Test sending a reply from the UI: for email replies, call the email send function; for LinkedIn, call PhantomBuster message send. Possibly integrate the AI “suggest reply”: hook up a button that calls OpenAI with the last message content and populates the textbox. Work on the UI styling of the chat interface (bubble styles, etc.) to make it clear and readable.
  * *Day 19:* **Testing and Quality Assurance:** Now we have most pieces built, spend this day as a buffer to test thoroughly with different scenarios:

    * Create a small campaign and simulate a positive reply (perhaps using a spare email account to actually reply and see if system catches it).
    * Test a campaign with LinkedIn steps (perhaps to a dummy LinkedIn account or just ensure the PhantomBuster call payload is correct).
    * Test approval mode: do messages wait for approval? Does approving trigger immediate send?
    * Ensure unsubscribe: reply “unsubscribe” and see if system stops further emails to that address (we should mark them opted-out).
    * Test error paths: invalid email credentials, ZoomInfo API limit reached (does it show an error to user?).
      Fix any bugs discovered (very likely in the integration points). Also check performance: a search with 50 results loading fine, etc. Ensure all API keys and secrets are correctly used from config and not leaking to client.
  * *Day 20:* **UI/UX Polishing:** Go through each page and refine the UI details:

    * Align elements properly, ensure mobile view is acceptable (Next.js by default can be responsive; test on a small screen).
    * Add any missing hints or labels (e.g., on hover over status icons, show “Connected on LinkedIn”, etc.).
    * Improve Loading states (maybe use skeleton screens for lead list loading, etc.).
    * Consistent formatting of names, dates (use a library like dayjs to format “2 days ago” etc. for last activity).
    * Add the brand styling – e.g., our logo (if any), or just a text logo for now. Pick a color for primary buttons and apply via theme.
    * Double-check the content AI writes – maybe limit how enthusiastic or length, adjust prompt accordingly.

* **Week 4: Buffer for Improvements, Documentation, and Deployment (Days 21-30)**

  * *Day 21-24:* With the core MVP functional, these days can be used to add any stretch features that we earlier put aside due to time:

    * **Website Visitors lead capture:** If not done yet, implement the script and API for WebSights/Clearbit Reveal. Possibly provide in Settings a script tag the user can copy. And a page in the app that lists recent site visitors and an “Add to campaign” button next to each. This might involve storing mapping of visitor -> company -> possible contacts (ZoomInfo might directly give some contacts from WebSights). Even if we just list the company name and let user click to search contacts at that company, that’s useful.
    * **Multiple Email Accounts & Rotation:** If the user’s email sending volume is high, sometimes rotating between multiple mailboxes under the domain helps. If the user has that setup (they said they have infrastructure), we could allow connecting several mail accounts and the system will round-robin sends. Implement if time permits.
    * **Multi-user support or Team features:** Possibly out of scope for 30 days, skip unless basic (maybe allow adding a teammate’s email as another sender, etc.).
    * **Reporting Dashboard:** If time, enhance dashboard with charts (e.g., line chart of daily emails sent vs replies). Use a chart library (Chart.js or Recharts) for a quick visualization.
    * **AI learning and improvement:** Possibly incorporate some feedback loop – e.g., if user edits an AI email a lot, feed that back into prompt next time (“the user prefers shorter emails”). This might be complex; a simpler approach: allow user to save an edited version as a custom template for future.
    * **Alternate data source integration:** If ZoomInfo is limited, maybe quickly plug in another API like **Hunter.io** for email finding if we only have name+domain. Or if user uploads their own list, allow CSV import as a campaign. These are auxiliary but could be considered if core work finished early.
  * *Day 25:* **Final end-to-end testing** with a realistic scenario: Use the platform as a user would – from onboarding to campaign to handling replies. Fine-tune any last UX issues (wording of messages, etc.). Ensure that the AI outputs are not offensive or incorrect (apply OpenAI content filter if needed to avoid any unsafe content in generation).
  * *Day 26:* **Deployment prep:** Set up the production environment variables for all service keys. Deploy the Next.js app to Vercel (or Supabase deploy if using their hosting). Migrate the database schema to production Supabase. Do a final smoke test on the deployed environment.
  * *Day 27-28:* **Documentation & User Guide:** Write brief documentation for the user (maybe a “Help” section or at least tooltips) explaining how to use the tool and best practices (like “It’s best to review the first few AI messages to ensure they align with your brand”, etc.). Also document internally how each integration works and any limitations (for maintenance).
  * *Day 29:* **Buffer for Unexpected Delays:** Use this day to handle any tasks that took longer than expected or any new bugs discovered after deployment. Polish the Figma mockups if needed for presentation (since the question explicitly wanted design mockups as part of the plan, we might prepare those to show stakeholders how the UI will look).
  * *Day 30:* **Launch & Monitor:** Make the platform live for the user. Closely monitor the first real campaign, check logs for any errors (especially API failures or rate limits). Be ready to quickly patch issues. Also gather initial feedback from the user – maybe they find the UI flow needs a tweak or the AI messages need adjusting. Use this to plan post-30-day improvements.

Throughout development, we’ll practice continuous integration – regularly pushing updates and testing incrementally to avoid last-minute surprises. We will use any available libraries or sample code from similar projects to speed up (for example, Clerk and Supabase have starter templates for Next.js that save time on setup).

By following this timeline, at the end of 30 days we expect to have a **fully functional AI SDR platform** that the user can use to generate leads and automate outreach. It will not be as feature-complete as a mature product (e.g., some advanced features of AiSDR like nuanced multi-turn conversations or deep CRM integrations might be out of scope), but it will cover end-to-end the main journey: from lead discovery to engaging them with personalized AI messages across email/LinkedIn, and capturing their responses inside the platform.

## Conclusion

In conclusion, this plan lays out how to rapidly build an **AI-driven SDR tool** using a lean tech stack and integration of powerful APIs. By focusing on core features and leveraging third-party services (ZoomInfo for data, OpenAI for content, PhantomBuster for LinkedIn, etc.), a single developer can implement a robust outbound automation platform in \~30 days. The product will embody the best aspects of existing AI SDR solutions – *a large lead database, automatic data enrichment, AI-personalized multi-channel outreach, and workflow automation* – while differentiating with a superior user experience and “AI agent” assistance at every step (from writing emails to replying to leads).

The end result will empower users to fill their sales pipeline on autopilot, as Artisan’s Ava does, but with more control and clarity. With the detailed design (including wireframes and the architecture diagram provided) and a structured implementation schedule, we can confidently proceed to development and bring this AI SDR platform to life.

**Sources:** The plan above was informed and inspired by insights from existing AI SDR platforms and relevant API documentation, including comparisons of AiSDR vs Artisan, reviews of Artisan AI’s features and user feedback, and technical capabilities of ZoomInfo’s API and PhantomBuster’s LinkedIn automation, among others. These references underscore the viability of features like extensive prospect data, personalized messaging, automated follow-ups, and integration-driven development within a short timeframe. Each cited source supports specific design decisions and feature justifications as detailed above.
